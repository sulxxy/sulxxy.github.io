title: DOCKS中文识别——基于Baidu + Sentencelist方法
date: 2015-09-11 20:03:37 
categories: Technique
tags: [DOCKS, NLP ]
---

# 引言
---
DOCKS的核心策略有二：
1. 将单词转化为音标进行识别。这是基于以下发现：
	- Google返回的结果中，单词的错误率远高于发音的错误率，比如”learn“总是会被识别为”Lauren“。
2. 采用了**后期处理**的方法，对Google的识别结果进行更深一步的处理。DOCKS中提供了三种后期处理方法：
	- Google + Sentencelist
	- Google + Sphinx
	- Google + Wordlist
<!-- more -->

通过以上两种策略，DOCKS有效地提高了识别的精度。遗憾的是，DOCKS本身并不能很好地支持中文识别。为了弥补这一缺陷，我们对DOCKS进行了一些扩展与补充，使其能够进行中文方面的识别。

本文着重介绍如何通过Baidu + Sentencelist的后期处理方式让DOCKS支持中文识别。

# 基本实现过程
---
## Baidu语音识别服务简介
由于Google的访问受限，所以将Google的那部分用百度的语音识别服务代替。最终的实验数据表明：在中文识别方面，Baidu的识别率要高于Google。同时，百度提供了更加完善的服务供开发者使用，诸如语义识别，上传自定义词库，语法模型等。基于以上几点原因，把Google识别模块替换成百度的服务理所应当。

## pinyin4j
DOCKS中，利用了Sphinx4中的G2PConverter类将单词转化为注音，知识库采用[亚琛工大开发的SequiturG2P](http://www-i6.informatik.rwth-aachen.de/web/Software/g2p.html)进行训练而得到。但SequiturG2P只能用于训练基于字母的语言，无法支持中文  。为此，可以采用一个开源项目[pinyin4j](http://pinyin4j.sourceforge.net/)的类库进行**汉字到拼音的转换**。拼音与注音存在细微差异，但这方面可以忽略不计。而且中文使用拼音的初衷也就是注音。因此，我们接下来的讨论中，拼音等价于注音。利用pinyin4j提供的类库，可以很容易的把汉字转化为拼音，经过这一步骤后，我们的后期处理就可以完全基于拼音来做。

## 知识库的建立
此种方法最为简单识别率也最高，知识库中存放的是域内所有可能出现的语句，以及这些语句转化成的注音的结果。
 
## 工作流程	
流程图如下：
![DOCKS Chinese Recognization](/img/DOCKS_Chinese_Recognization.png)
	
首次初始化时，知识库中的所有语句会被转化为注音存放在.ser文件中。对于输入的音频流，首先会被发送到谷歌进行初步识别，返回的结果集同样会被pinyin4j转化为注音。通过Levenshtein算法，转化为注音的结果会与知识库（.ser文件）中的语句一一比对，获取那个差距最小（即最优）值作为识别结果。

# 结果分析
---
一共测试了19202组数据。测试有四种情况，分别是：
- 没有自定义词库，没有后期处理；
- 没有自定义词库，有后期处理；
- 有自定义词库，没有后期处理；
- 有自定义词库，有后期处理。

统计结果如下，四种情况识别出错的个数分别是：4653次,368次,4602次,352次。错误率对应分别是：24.232%，1.916%，23.966%，1.833%。这错误还包括“一支笔”被识别成“一只笔”这种无关紧要的错误。数据中可以看到：
- 上传到百度服务器的自定义词库基本没有起到特别显著的作用；
- 后期处理的作用十分巨大。

# 与DOCKS中的Google + Sentencelist的方法的联系与区别
---
二者的核心思路完全相同：知识库中枚举了域内所有可能出现的语句，将输入的语句在知识库中一一比对，获取识别结果。

区别在于：转化成注音的方式不一样，原始的DOCKS采用的SequiturG2P生成，中文识别部分采用的是pinyin4j。这一差别导致了在知识库的结构上二者的差异。采用SequiturG2P生成的知识库，里面计算好了所有最优路径，而中文的知识库没有这一操作，仅仅是所有语句的注音的枚举。相比之下，中文识别的效率可能要低于英文。

title: DOCKS Question list
date: 2015-08-30 10:44:01

tags: [ DOCKS, NLP ]
---
 
# Purpose
---
在NAO机器人上进行声音采集，可以使其能够实时的进行声音方位的定位和内容的识别，甚至包括是哪个人说话的识别。
<!-- more -->
	 
# Questions
---
1.  Sphinx没有好的声学模型，那么它里面是怎么样解决的？这是一个核心，它的解决思路的出发点是什么，这个思路中有什么可借鉴的，有什么可改进的？
Sphinx没有好的声学模型，但它有可被训练的功能。在小规模的应用场景中，用户完全可以根据自己的需求，利用CMU提供一些其他工具训练出自己的声学模型来使用。同时，还包括语法模型，词典等，都可以通过CMU的工具生成并在Sphinx中使用。
2. 如何应用？在实际使用中，机器人进行实时声音采集，向后台发送，调用，识别出来的结果再反馈，这个过程中有哪些问题存在？
语音识别只是第一步，<font color="red">需要做的另一个重要的工作是语义识别</font>，如果采用Sentencelist的方式的话语义识别会很简单，其他方式会相对复杂一些。单就语音识别这部分来说，在**精度达到要求的前提下**，识别速度是目前存在的一个主要的问题。从目前测试的结果看，识别效果并不理想，导致这个结果有两方面因素：其一，输入动作检测还存在细微的问题，比如一段话会被自动断句为两个输入（不过是小概率）；其二，网络方面的制约，也是最主要的方面。DOCKS所采用的方式是处在一个分布式的环境中的，这其中向谷歌服务器发送请求再返回会耗费不少的时间。而且由于音频采集部分存在的一些瑕疵，整体的识别率还有优化提高的空间。（2015-09-20 15:05:47 补充：需要提及的是，在将谷歌语音识别服务替换为百度语音识别服务后，这两个问题基本得到了解决，看来还是追根溯源还是网络的问题。）
3. 对中文的支持如何？
中文是可以支持的。将代码中发给谷歌的那个链接中的参数改为了中文对应的zh-CN，然后进行识别，程序会异常退出。改起来很简单，错误出在两个地方：一方面是在谷歌返回的结果中，识别的部分会是乱码，这部分可以在创建输入流对象时加个utf-8参数；另一方面是项目对结果集进行处理时用到了正则表达式来获取谷歌返回的结果中真正有用的部分，在这里删除了所有除了字母和数字以外的所有其他字符，把这部分修改即可。通过更改以上两部分，即可完成对中文的识别（2015-09-20 15:06:02 补充：当然也可以直接不用谷歌的服务，换用百度的）。当然这只是谷歌的部分，如果想让DOCKS真正支持中文，需要做的另外一个重要的工作是把中文字符转化为注音，这样方能进行注音级别的比对筛选。这部分已经基本解决，详细可见笔者另一篇文章：[ DOCKS中文识别——基于Baidu + Sentencelist方法](http://sulxxy.github.io/2015/09/11/DOCKS_Chinese_Recognization/)
4. 在测试中发现了什么问题？或者有什么改进的思路？
目前测试中一个主要的问题是在复杂环境下采集声音的精度不高，噪音总是会被当做正常输入采集进来。同时，断句识别也不大精准，有时候一个长句子中间有断句停顿，基本到前半句读完后就会被识别为输入结束。这两点，由于都和sphinx的Frontend有关，处理起来有些麻烦。目前处理了断句识别的问题，可参照笔者的另一篇文章：[DOCKS中对声音采集部分的优化](http://sulxxy.github.io/2015/08/30/DOCKS%20Audio%20Colletcion%20optimization/)。（2015-09-20 14:56:22 补充： 后来换成百度以后，发现就并没有这种问题，出现这种问题的根源很可能在于网络。）

在以下情况中，会有效率方面的问题：比如因为某个因素谷歌返回的结果只识别了语句的一半（且准确度很高），那么根据sentencelist等方法返回的结果精度会出问题，因为比对结果是按整句来进行比对，可能会出现识别结果是句子长度较为相近但内容差别较大。比如原句为：something is behind me something is approching me. 谷歌识别的结果为：something is behind me. 那么通过后期处理方法返回的可能会是door is in your back.虽然句法相近，但含义已经差别巨大。<font color="red">关于这部分，问题主要集中在基于sentencelist的方法中，采用Levenshtein算法必然会出现这种尴尬的情况。目前还没能想到好的解决办法。</font>
5.  要仔细弄清楚这两个开源软件各自的功能特性，优缺点，如果应用到机器人上，进行语音交互，其可实现的功能有哪些？能不能实现更加智能化的功能，比如：通过语音的情绪判断分析，语音的分类（针对人），噪声环境下的识别等。	
两者的主要功能都是在某个特定的域内进行语音识别。Sphinx具有很好的灵活性和可拓展性。其中有一个重要的特性就是可插拔的前端，在实际工作过程中前端内部的实际内容只能在和词典、模型等的比对阶段才会被访问。DOCKS正是利用了Sphinx的这一特性对其进行了改良，实现了自己的自定义前端，并结合Google的ASR服务实现了更加精确高效的识别率。
6. 关于更加智能化的功能：
	- 情绪判断：<font color="red">这方面目前老师给出了两个开源可供研究： [openAUDIO.eu](http://openaudio.eu/); [BEYONDVERBAL](http://www.beyondverbal.com/choose-solution/emotions-analytics-api/)</font>。
	- 噪声环境下的识别也是不理想的。在声音动作检测那部分出了问题，噪音会被当做正常输入录入。关于这方面，目前没有一种很好的解决办法。在Sphinx的文档中对这部分有所提及，[Voice Activity Detection](http://cmusphinx.sourceforge.net/wiki/asr:vad)。
	- 语音的分类目前已经在DOCKS上实现，主要是基于对Sphinx中相关类库的修改，可参照笔者的另一篇文章：[SpeakerIdentification in DOCKS](http://sulxxy.github.io/2015/08/30/DOCKS%20speakeridentify/)。
7. 从给你的那篇德国论文角度出发，考虑他们研究的出发点，所解决的核心问题是什么？
	- 出发点：
		- 对于那些应用广泛的语音识别服务，无法在限定的域内很好地工作，出错率甚至高于一些开源的语音识别项目。当然开源的语音识别项目的缺点是没有好的声学模型。
		- 同时，经过DOCKS项目组研究发现，从谷歌返回的结果中，音标识别的错误率要远低于单词识别的错误率。于是DOCKS使用了基于注音的识别方法。
	- **DOCKS项目所解决的核心问题**就是如何兼容二者的优点，摒弃二者的劣势，以提高识别率。下面列出Google的ASR服务和CMU的Sphinx项目各自的特点：
		- **Google ASR**：
			- 声学模型更好，识别精度高
			- 搜索范围广泛
			- 无法限定域
		- **Sphinx**：
			-  灵活，可扩展性高
			-  没有好的声学模型
			- 可靠性较低
7. DOCKS是如何结合Google ASR和CMU的Sphinx4的？在这个结合中，可以实现哪些功能？哪些功能不能实现？
提及如何二者如何结合之前，有必要对Sphinx的前端做简要介绍。Sphinx的前端是一系列数据处理器的集合，输入数据依次流经这些数据处理器，即可输出最终前端的处理后的结果并交由decoder模块处理（<font color="red">decoder模块还说不清楚，不大清楚内部实现细节</font>），这样一种设计方式使得前端具有很大的灵活性，DOCKS正是利用了这个特点。关于Frontend更多的内容请参见笔者的另一篇文章：[Sphinx--Frontend分析](http://sulxxy.github.io/2015/09/02/Docks%20Frontend/)
 ASR与Sphinx4结合的识别方法流程图如下：
	![SphinxBasedPostProcessor流程分析](/img/DOCKSSphinxBased.png)

如图所示，为了有效地结合Google ASR和Sphinx，DOCKS首先把配置文件中的Frontend部分替换成了自定义的PhoneFrontEnd，此步骤是为了让前端中存储的数据是Google识别结果的注音。在此后的过程中，则全部交由Sphinx来处理。

<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.1">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"liuzhiwei.me","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"right","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="IntroductionThis post mainly discusses the difference between gradient gescent (a.k.a. vanilla gradient descent or batch gradient descent) and stochastic gradient descent (a.k.a. online gradient desce">
<meta property="og:type" content="article">
<meta property="og:title" content="Difference Between Gradient Descent and Stochastic Gradient Descent">
<meta property="og:url" content="http://liuzhiwei.me/2020/07/05/Diff_SGD_and_GD/index.html">
<meta property="og:site_name" content="COMMENTARIUM">
<meta property="og:description" content="IntroductionThis post mainly discusses the difference between gradient gescent (a.k.a. vanilla gradient descent or batch gradient descent) and stochastic gradient descent (a.k.a. online gradient desce">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2020-07-05T04:17:49.833Z">
<meta property="article:modified_time" content="2020-06-14T11:50:27.160Z">
<meta property="article:author" content="Zhiwei Liu">
<meta property="article:tag" content="Machine Learning">
<meta property="article:tag" content="Optimization Theory">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://liuzhiwei.me/2020/07/05/Diff_SGD_and_GD/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>Difference Between Gradient Descent and Stochastic Gradient Descent | COMMENTARIUM</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">COMMENTARIUM</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://liuzhiwei.me/2020/07/05/Diff_SGD_and_GD/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zhiwei Liu">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="COMMENTARIUM">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Difference Between Gradient Descent and Stochastic Gradient Descent
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-07-05 12:17:49" itemprop="dateCreated datePublished" datetime="2020-07-05T12:17:49+08:00">2020-07-05</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-06-14 19:50:27" itemprop="dateModified" datetime="2020-06-14T19:50:27+08:00">2020-06-14</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine_Learning</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>This post mainly discusses the difference between gradient gescent (a.k.a. vanilla gradient descent or batch gradient descent) and stochastic gradient descent (a.k.a. online gradient descent). We firstly show and compare the 2 alogorithms’ implementations, which is quite simple. After that, we will analyse these 2 algorithms from the convergence rate perspective. At the end, the summaries are given.</p>
<a id="more"></a>

<h1 id="Algorithm-Implementation"><a href="#Algorithm-Implementation" class="headerlink" title="Algorithm Implementation"></a>Algorithm Implementation</h1><p>In this part, we mainly concentrate on the implementation of both algorithms to get an intuive difference between them. Assumes we have loss function $L(\mathbf{w}; \mathbf{x})$, in which $\mathbf{w}$ is the parameters to train and $\mathbf{x}$ is the input data.<br>For gradient descent (GD),</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient_descent</span><span class="params">(loss, w, T, X, eta)</span>:</span></span><br><span class="line">    :param loss: the loss function</span><br><span class="line">    :param w: trainable parameters</span><br><span class="line">    :param T: iterations to train</span><br><span class="line">    :param X: data points</span><br><span class="line">    :param eta: the learning rate</span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(T):</span><br><span class="line">        gradient_sum = []</span><br><span class="line">        <span class="keyword">for</span> x <span class="keyword">in</span> X:</span><br><span class="line">            cur_gradient = loss.subgradient(w, x)</span><br><span class="line">            gradient_sum = element_wise_add(gradient_sum, cur_gradient)</span><br><span class="line">        gradient = gradient_sum/X.size()</span><br><span class="line">        w = w - eta * gradient</span><br><span class="line">    <span class="keyword">return</span> w</span><br></pre></td></tr></table></figure>
<p>Note that in order to show the main difference with later SGD algorithm, above code snippet is not optimized (i.e. vectorized).</p>
<p>For stochastic gradient descent (SGD), </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">stochastic_gradient_descent</span><span class="params">(loss, w, T, X, eta)</span>:</span></span><br><span class="line">    :param loss: the loss function</span><br><span class="line">    :param w: trainable parameters</span><br><span class="line">    :param T: iterations to train</span><br><span class="line">    :param X: data points</span><br><span class="line">    :param eta: the learning rate</span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(T):</span><br><span class="line">        x = rand_choose(X)</span><br><span class="line">        cur_gradient = loss.subgradient(w, x)</span><br><span class="line">        w = w - eta * gradient</span><br><span class="line">    <span class="keyword">return</span> w</span><br></pre></td></tr></table></figure>
<p>As we could see from above 2 pseudo code snippets, the key difference is the SGD randomly chooses a data point to optimize, while the GD optimize on the whole dataset. As a result, the time complexity of GD and SGD are $\mathcal{O}(TN)$<br>(without considering vectorization) and $\mathcal{O}(T)$, respectively. Both implementations are simple and easily understood. </p>
<h1 id="Analysis"><a href="#Analysis" class="headerlink" title="Analysis"></a>Analysis</h1><p>This part analyses the convergence rate of the 2 algorithms mathematically. Since nonconvex optimization problems are extremely complex (NP-hard), we only discuss convex optimization here, which assumes the loss functions are convex.<br>The simplest case is that loss function $f(\mathbf{x})$ is $\gamma$-well-conditioned (which means the function is both $\alpha$-strongly convex and $\beta$-smooth).</p>
<h2 id="Gradient-Descent"><a href="#Gradient-Descent" class="headerlink" title="Gradient Descent"></a>Gradient Descent</h2><p>Set $h_t = f(\mathbf{x}_t) - f(\mathbf{x}^{*})$, where $\mathbf{x}^*$ is the (unknown) optimal point. we will show that for unconstrained minimization: </p>
<p>$$h_{t+1} \leq h_{1}e^{-\gamma t}$$</p>
<p>in which $\eta_t = \frac{1}{\beta}$.</p>
<p>According to strongly convexity, for any $\mathbf{x}, \mathbf{y} \in \mathcal{K}$:<br>$$<br>\begin{align}<br>f(\mathbf{y}) &amp; \geq f(\mathbf{x}) + \nabla f(\mathbf{x})^\top (\mathbf{y}-\mathbf{x}) + \frac{\alpha}{2} \Vert\mathbf{x} - \mathbf{y}\Vert^2 \<br>&amp; \geq \min_{\mathbf{z}}\Bigl\{f(\mathbf{x}) + \nabla f(\mathbf{x})^\top (\mathbf{z}-\mathbf{x}) + \frac{\alpha}{2} \Vert\mathbf{x} - \mathbf{z}\Vert^2\Bigr\} \<br>&amp; = f(\mathbf{x}) - \frac{1}{2\alpha}\Vert\nabla f(\mathbf{x})\Vert^2<br>\end{align}<br>$$</p>
<p>In particular, take $\mathbf{x} = \mathbf{x}_t$ and $\mathbf{y} = \mathbf{x}^*$,</p>
<p>$$<br>\nabla f(\mathbf{x}_t) \geq 2\alpha(f(\mathbf{x}_t) - f(\mathbf{x}^*)) = 2\alpha h_t<br>$$</p>
<p>Meanwhile, according to the $\beta$-smoothness,</p>
<p>$$<br>\begin{align}<br>h_{t+1} - h_t &amp; = f(\mathbf{x}_{t+1}) - f(\mathbf{x}_t) \<br>&amp; \leq \nabla f(\mathbf{x}_t)^\top (\mathbf{x}_{t+1} - \mathbf{x}_{t}) + \frac{\beta}{2}\Vert \mathbf{x}_{t+1} - \mathbf{x}_t\Vert^2 \<br>&amp; = -\eta_t \Vert \nabla f(\mathbf{x}_t) \Vert^2 + \frac{\beta}{2} \eta_t^2 \Vert \nabla f(\mathbf{x}_t) \Vert^2 \<br>&amp; = - \frac{1}{2\beta}\Vert \nabla f(\mathbf{x}_t) \Vert^2 \<br>&amp; \leq -\frac{\alpha}{\beta}h_t<br>\end{align}<br>$$</p>
<p>Thus,</p>
<p>$$h_{t+1} \leq h_{t}(1-\frac{\alpha}{\beta} \leq \cdots \leq h_{1}(1-\gamma)^t \leq h_{1}e^{-\gamma t} \tag{2} \label{eq:2}$$</p>
<p>In order to make it comparable with later SGD analysis, we compute the <em>regret</em> $R_T = \sum_{t=1}^T f(\mathbf{x}_t) - \min_{x^* \in \mathcal{K}}\sum_{t=1}^T f(\mathbf{x}^*) = \sum_{t=1}^T h_t$, according to $\ref{eq:2}$, </p>
<p>$$R_T \leq \frac{h_1}{e^\gamma - 1} (e^{-\gamma T} - 1) = \mathcal{O}(e^{-\gamma T})$$</p>
<h2 id="Stochastic-Gradient-Descent"><a href="#Stochastic-Gradient-Descent" class="headerlink" title="Stochastic Gradient Descent"></a>Stochastic Gradient Descent</h2><p>SGD is an online optimizer, we use <em>regret</em> as measurement. Different with former mentioned formula, SGD’s regret is written as:</p>
<p>$$R_T = \sum_{t=1}^T f_t(\mathbf{x}_t) - \min_{\mathbf{x}^* \in \mathcal{K}} \sum_{t=1}^T f_t(\mathbf{x}^*)$$</p>
<p>in which the fuction $f_t({\mathbf{x}})$ is different at each iteration since $\mathbf{x}$ are randomly chosed in each iteration in this algorithm. We can prove that </p>
<p>$$<br>R_T \leq \frac{3}{2}GD\sqrt{T}<br>$$<br>where $G$ is the lipschitz constant and $D$ is the diameter of decision set.</p>
<p>The 2 algorithms reprensents 2 different optimization schemes: offline optimization (GD) and online optimization (SGD).</p>
<h1 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h1><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p>[1] E. Hazan, “Introduction to Online Convex Optimization,” Found. Trends® Optim., vol. 2, no. 3–4, pp. 157–325, 2016.</p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Machine-Learning/" rel="tag"># Machine Learning</a>
              <a href="/tags/Optimization-Theory/" rel="tag"># Optimization Theory</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2018/01/30/SVM/" rel="prev" title="Support Vector Machine">
      <i class="fa fa-chevron-left"></i> Support Vector Machine
    </a></div>
      <div class="post-nav-item">
    <a href="/2020/07/05/Back_Propagation/" rel="next" title="Backpropagation Algorithm">
      Backpropagation Algorithm <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Introduction"><span class="nav-number">1.</span> <span class="nav-text">Introduction</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Algorithm-Implementation"><span class="nav-number">2.</span> <span class="nav-text">Algorithm Implementation</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Analysis"><span class="nav-number">3.</span> <span class="nav-text">Analysis</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Gradient-Descent"><span class="nav-number">3.1.</span> <span class="nav-text">Gradient Descent</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Stochastic-Gradient-Descent"><span class="nav-number">3.2.</span> <span class="nav-text">Stochastic Gradient Descent</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Summary"><span class="nav-number">4.</span> <span class="nav-text">Summary</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Reference"><span class="nav-number">5.</span> <span class="nav-text">Reference</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Zhiwei Liu</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">9</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/sulxxy" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;sulxxy" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Zhiwei Liu</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://mist.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>

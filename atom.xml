<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>COMMENTARIUM</title>
  
  
  <link href="http://www.liuzhiwei.me/atom.xml" rel="self"/>
  
  <link href="http://www.liuzhiwei.me/"/>
  <updated>2022-08-19T04:07:45.094Z</updated>
  <id>http://www.liuzhiwei.me/</id>
  
  <author>
    <name>Zhiwei Liu</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>auto in CPP</title>
    <link href="http://www.liuzhiwei.me/auto_in_cpp/"/>
    <id>http://www.liuzhiwei.me/auto_in_cpp/</id>
    <published>2022-08-17T21:43:11.000Z</published>
    <updated>2022-08-19T04:07:45.094Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>auto&amp;, auto</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Introduction&quot;&gt;&lt;a href=&quot;#Introduction&quot; class=&quot;headerlink&quot; title=&quot;Introduction&quot;&gt;&lt;/a&gt;Introduction&lt;/h1&gt;&lt;p&gt;auto&amp;amp;, auto&lt;/p&gt;
</summary>
      
    
    
    
    <category term="programming_languages" scheme="http://www.liuzhiwei.me/categories/programming-languages/"/>
    
    
    <category term="CPP" scheme="http://www.liuzhiwei.me/tags/CPP/"/>
    
  </entry>
  
  <entry>
    <title>Write A Random Generator By Yourself</title>
    <link href="http://www.liuzhiwei.me/random_generator/"/>
    <id>http://www.liuzhiwei.me/random_generator/</id>
    <published>2022-08-15T21:43:11.000Z</published>
    <updated>2022-08-19T04:07:45.094Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Introduction&quot;&gt;&lt;a href=&quot;#Introduction&quot; class=&quot;headerlink&quot; title=&quot;Introduction&quot;&gt;&lt;/a&gt;Introduction&lt;/h1&gt;</summary>
      
    
    
    
    <category term="Math" scheme="http://www.liuzhiwei.me/categories/Math/"/>
    
    
    <category term="CPP" scheme="http://www.liuzhiwei.me/tags/CPP/"/>
    
    <category term="Math" scheme="http://www.liuzhiwei.me/tags/Math/"/>
    
  </entry>
  
  <entry>
    <title>Singleton in CPP</title>
    <link href="http://www.liuzhiwei.me/singleton_in_cpp/"/>
    <id>http://www.liuzhiwei.me/singleton_in_cpp/</id>
    <published>2022-08-15T21:43:11.000Z</published>
    <updated>2022-08-19T04:07:45.094Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">std::once_flag</span><br><span class="line">std::call_once</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Introduction&quot;&gt;&lt;a href=&quot;#Introduction&quot; class=&quot;headerlink&quot; title=&quot;Introduction&quot;&gt;&lt;/a&gt;Introduction&lt;/h1&gt;&lt;figure class=&quot;highlight c++&quot;&gt;&lt;ta</summary>
      
    
    
    
    <category term="Design_Patterns" scheme="http://www.liuzhiwei.me/categories/Design-Patterns/"/>
    
    
    <category term="CPP" scheme="http://www.liuzhiwei.me/tags/CPP/"/>
    
  </entry>
  
  <entry>
    <title>Publish Your WordPress Post in Automation with GitHub Actions</title>
    <link href="http://www.liuzhiwei.me/automated_wordpress_post/"/>
    <id>http://www.liuzhiwei.me/automated_wordpress_post/</id>
    <published>2022-08-15T18:43:11.000Z</published>
    <updated>2022-08-19T04:07:45.094Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Python script:</p><ol><li>markdown with mathjax</li><li>convert markdown to html</li><li>Post to WordPress via restful <a href="https://developer.wordpress.org/rest-api/reference/">API</a></li></ol><p>Trigger the above script through GitHub action.</p><ol><li>setup application password</li><li>Optional: only trigger action for specific commit messages</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Introduction&quot;&gt;&lt;a href=&quot;#Introduction&quot; class=&quot;headerlink&quot; title=&quot;Introduction&quot;&gt;&lt;/a&gt;Introduction&lt;/h1&gt;&lt;p&gt;Python script:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;ma</summary>
      
    
    
    
    <category term="tools" scheme="http://www.liuzhiwei.me/categories/tools/"/>
    
    
  </entry>
  
  <entry>
    <title>EM Algorithm and Gaussian Mixture Model</title>
    <link href="http://www.liuzhiwei.me/EM_GMM/"/>
    <id>http://www.liuzhiwei.me/EM_GMM/</id>
    <published>2017-11-23T11:43:11.000Z</published>
    <updated>2022-08-19T04:07:45.094Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Introduction-to-EM"><a href="#Introduction-to-EM" class="headerlink" title="Introduction to EM"></a>Introduction to EM</h1><p>Previously I talked about <a href="http://liuzhiwei.me/Bayes_Classifier/">Naive Bayes Classifier</a>. In that case, all variables are observable. However, sometimes there could be some latent variables in some models. EM(Expection-Maximization) algorithm could resolve those models containing latent variables.</p><span id="more"></span><h1 id="Gaussian-Mixture-Models"><a href="#Gaussian-Mixture-Models" class="headerlink" title="Gaussian Mixture Models"></a>Gaussian Mixture Models</h1><p>Before talking general case of EM algorithm, we introduce the Gaussian Mixture Models(GMM) and calculate its parameters using EM.</p><h2 id="What-is-GMM"><a href="#What-is-GMM" class="headerlink" title="What is GMM?"></a>What is GMM?</h2><p>GMM is a composition of several components with Gaussian distribution and different components. It could be written like,<br>$$p(x) :&#x3D; \sum_{k&#x3D;1}^K \pi_k\mathcal{N}(x|\mu_k, , \Sigma_k) \tag{1}\label{eq:GMM}$$</p><p>Simply speaking, we can explain $\pi_k$ as the weights of each Gaussian.</p><p>However, in order to understand what the latent variable is and how it works, first we explain the formulation of the equation $\ref{eq:GMM}$ from the point of latent variables.</p><p>Imaging we have a K-dimensional binary vector $\mathcal{z}$ in which the $k^{th}$ element $\mathcal{i_k}$ equal to $\mathcal{1}$ and others are $\mathcal{0}$. Now we need to define $p(x, z)$:<br>$$p(x, z) :&#x3D; p(x|z)\cdot p(z)\tag{2}\label{eq:2}$$</p><p>in equation $\ref{eq:2}$, we need to consider $p(z)$ and $p(x|z)$. </p><ol><li><p>$p(z)$</p><p>$p(z)$ is the marginal distribution of $p(x, z)$ over $\mathcal{z}$, assuming it equals to the mixing coefficients $\pi_k$, so we have:<br>$$p(z_k &#x3D; 1) :&#x3D; \pi_k\tag{3}\label{eq:3}$$</p><p>since in vector $\mathcal{z}$, all values except \(\mathcal{z_k}\) are $\mathcal{0}$, so equation $\ref{eq:3}$ can be written like:</p><p>$$p(z) :&#x3D; \prod_{k&#x3D;1}^K \pi_k^{z_k}\tag{4}\label{eq:4}$$</p></li><li><p>$p(x|z)$</p><p>Also, assuming that the conditional distribution of $\mathcal{x}$ given a particular value for $\mathcal{z}$ is a Gaussian,<br>$$p(x|z_k&#x3D;1) :&#x3D; \mathcal{N}(x|\mu_k, , \Sigma_k)\tag{5}\label{eq:5}$$</p><p>Similarly, equation $\ref{eq:5}$ can be written like:<br>$$p(x|z) :&#x3D; \prod_{k&#x3D;1}^K \mathcal{N}(x|\mu_k, , \Sigma_k)^{z_k}\tag{6}\label{eq:6}$$</p></li></ol><p>Now we can calculate the marginal distribution of $p(x, z)$ over $\mathcal{x}$:<br>$$p(x) :&#x3D; \sum_{z} p(z)\cdot p(x|z)\tag{7}\label{eq:7}$$</p><p>put equation $\ref{eq:4}$ and $\ref{eq:6}$ into equation $\ref{eq:7}$,<br>$$p(x) :&#x3D; \sum_{k&#x3D;1}^K \pi_k \mathcal{N}(x|\mu_k, , \Sigma_k)\tag{8}\label{eq:8}$$</p><p>Equation $\ref{eq:8}$ is exactly same with equation $\ref{eq:GMM}$, so now we succefully explained the GMM from the point of latent variables.</p><p>Next, we need to calculate the conditional probability of $\mathcal{z}$ given $\mathcal{x}$, let us call it $\gamma(z_k)$,<br>$$\gamma(z_k) :&#x3D; p(z_k&#x3D;1|x) :&#x3D; \frac {p(z_k&#x3D;1)\cdot p(x|z_k&#x3D;1)}{p(x)}\tag{9}\label{eq:9}$$</p><p>put equation $\ref{eq:3}$, $\ref{eq:5}$ and $\ref{eq:8}$ into equation $\ref{eq:9}$,<br>$$\gamma(z_k) :&#x3D; \frac {\pi_k\mathcal{N}(\mu_k,,\Sigma_k)}{\sum_{j&#x3D;1}^K\mathcal{N}(x|\mu_j, , \Sigma_j)}\tag{10}\label{eq:10}$$</p><p>To prepare for the explaination using EM algorithm next, one important thing has to be emphasized:</p><ol><li>in equation $\ref{eq:8}$, $\pi_k$ should be seen as the <strong>prior</strong> of $z_k&#x3D;1$(this is what equation $\ref{eq:3}$ is saying);</li><li>in equation $\ref{eq:10}$, $\gamma(z_k)$ should be seen as <strong>posterior</strong> once we obeserved $\mathcal{x}$(this is what equation $\ref{eq:9}$ saying).</li></ol><h2 id="EM-algorithm-on-GMM"><a href="#EM-algorithm-on-GMM" class="headerlink" title="EM algorithm on GMM"></a>EM algorithm on GMM</h2><h1 id="EM-algorithm-in-general"><a href="#EM-algorithm-in-general" class="headerlink" title="EM algorithm in general"></a>EM algorithm in general</h1><h1 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h1><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li>Bishop: Pattern Recognition and Machine Learning, Springer-Verlag, 2006</li><li>李航：统计学习方法，清华大学出版社，2012</li></ol><h1 id="related-Articles"><a href="#related-Articles" class="headerlink" title="related Articles"></a>related Articles</h1><ol start="0"><li>Bayesian Theory</li><li>K-means</li></ol>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;Introduction-to-EM&quot;&gt;&lt;a href=&quot;#Introduction-to-EM&quot; class=&quot;headerlink&quot; title=&quot;Introduction to EM&quot;&gt;&lt;/a&gt;Introduction to EM&lt;/h1&gt;&lt;p&gt;Previously I talked about &lt;a href=&quot;http://liuzhiwei.me/Bayes_Classifier/&quot;&gt;Naive Bayes Classifier&lt;/a&gt;. In that case, all variables are observable. However, sometimes there could be some latent variables in some models. EM(Expection-Maximization) algorithm could resolve those models containing latent variables.&lt;/p&gt;</summary>
    
    
    
    <category term="Machine_Learning" scheme="http://www.liuzhiwei.me/categories/Machine-Learning/"/>
    
    
    <category term="Machine Learning" scheme="http://www.liuzhiwei.me/tags/Machine-Learning/"/>
    
  </entry>
  
</feed>

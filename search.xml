<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>EM Algorithm and Gaussian Mixture Model</title>
    <url>/EM_GMM/</url>
    <content><![CDATA[<h1 id="Introduction-to-EM"><a href="#Introduction-to-EM" class="headerlink" title="Introduction to EM"></a>Introduction to EM</h1><p>Previously I talked about <a href="http://liuzhiwei.me/Bayes_Classifier/">Naive Bayes Classifier</a>. In that case, all variables are observable. However, sometimes there could be some latent variables in some models. EM(Expection-Maximization) algorithm could resolve those models containing latent variables.</p>
<span id="more"></span>

<h1 id="Gaussian-Mixture-Models"><a href="#Gaussian-Mixture-Models" class="headerlink" title="Gaussian Mixture Models"></a>Gaussian Mixture Models</h1><p>Before talking general case of EM algorithm, we introduce the Gaussian Mixture Models(GMM) and calculate its parameters using EM.</p>
<h2 id="What-is-GMM"><a href="#What-is-GMM" class="headerlink" title="What is GMM?"></a>What is GMM?</h2><p>GMM is a composition of several components with Gaussian distribution and different components. It could be written like,<br>$$p(x) :&#x3D; \sum_{k&#x3D;1}^K \pi_k\mathcal{N}(x|\mu_k, , \Sigma_k) \tag{1}\label{eq:GMM}$$</p>
<p>Simply speaking, we can explain $\pi_k$ as the weights of each Gaussian.</p>
<p>However, in order to understand what the latent variable is and how it works, first we explain the formulation of the equation $\ref{eq:GMM}$ from the point of latent variables.</p>
<p>Imaging we have a K-dimensional binary vector $\mathcal{z}$ in which the $k^{th}$ element $\mathcal{i_k}$ equal to $\mathcal{1}$ and others are $\mathcal{0}$. Now we need to define $p(x, z)$:<br>$$p(x, z) :&#x3D; p(x|z)\cdot p(z)\tag{2}\label{eq:2}$$</p>
<p>in equation $\ref{eq:2}$, we need to consider $p(z)$ and $p(x|z)$. </p>
<ol>
<li><p>$p(z)$</p>
<p>$p(z)$ is the marginal distribution of $p(x, z)$ over $\mathcal{z}$, assuming it equals to the mixing coefficients $\pi_k$, so we have:<br>$$p(z_k &#x3D; 1) :&#x3D; \pi_k\tag{3}\label{eq:3}$$</p>
<p>since in vector $\mathcal{z}$, all values except \(\mathcal{z_k}\) are $\mathcal{0}$, so equation $\ref{eq:3}$ can be written like:</p>
<p>$$p(z) :&#x3D; \prod_{k&#x3D;1}^K \pi_k^{z_k}\tag{4}\label{eq:4}$$</p>
</li>
<li><p>$p(x|z)$</p>
<p>Also, assuming that the conditional distribution of $\mathcal{x}$ given a particular value for $\mathcal{z}$ is a Gaussian,<br>$$p(x|z_k&#x3D;1) :&#x3D; \mathcal{N}(x|\mu_k, , \Sigma_k)\tag{5}\label{eq:5}$$</p>
<p>Similarly, equation $\ref{eq:5}$ can be written like:<br>$$p(x|z) :&#x3D; \prod_{k&#x3D;1}^K \mathcal{N}(x|\mu_k, , \Sigma_k)^{z_k}\tag{6}\label{eq:6}$$</p>
</li>
</ol>
<p>Now we can calculate the marginal distribution of $p(x, z)$ over $\mathcal{x}$:<br>$$p(x) :&#x3D; \sum_{z} p(z)\cdot p(x|z)\tag{7}\label{eq:7}$$</p>
<p>put equation $\ref{eq:4}$ and $\ref{eq:6}$ into equation $\ref{eq:7}$,<br>$$p(x) :&#x3D; \sum_{k&#x3D;1}^K \pi_k \mathcal{N}(x|\mu_k, , \Sigma_k)\tag{8}\label{eq:8}$$</p>
<p>Equation $\ref{eq:8}$ is exactly same with equation $\ref{eq:GMM}$, so now we succefully explained the GMM from the point of latent variables.</p>
<p>Next, we need to calculate the conditional probability of $\mathcal{z}$ given $\mathcal{x}$, let us call it $\gamma(z_k)$,<br>$$\gamma(z_k) :&#x3D; p(z_k&#x3D;1|x) :&#x3D; \frac {p(z_k&#x3D;1)\cdot p(x|z_k&#x3D;1)}{p(x)}\tag{9}\label{eq:9}$$</p>
<p>put equation $\ref{eq:3}$, $\ref{eq:5}$ and $\ref{eq:8}$ into equation $\ref{eq:9}$,<br>$$\gamma(z_k) :&#x3D; \frac {\pi_k\mathcal{N}(\mu_k,,\Sigma_k)}{\sum_{j&#x3D;1}^K\mathcal{N}(x|\mu_j, , \Sigma_j)}\tag{10}\label{eq:10}$$</p>
<p>To prepare for the explaination using EM algorithm next, one important thing has to be emphasized:</p>
<ol>
<li>in equation $\ref{eq:8}$, $\pi_k$ should be seen as the <strong>prior</strong> of $z_k&#x3D;1$(this is what equation $\ref{eq:3}$ is saying);</li>
<li>in equation $\ref{eq:10}$, $\gamma(z_k)$ should be seen as <strong>posterior</strong> once we obeserved $\mathcal{x}$(this is what equation $\ref{eq:9}$ saying).</li>
</ol>
<h2 id="EM-algorithm-on-GMM"><a href="#EM-algorithm-on-GMM" class="headerlink" title="EM algorithm on GMM"></a>EM algorithm on GMM</h2><h1 id="EM-algorithm-in-general"><a href="#EM-algorithm-in-general" class="headerlink" title="EM algorithm in general"></a>EM algorithm in general</h1><h1 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h1><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol>
<li>Bishop: Pattern Recognition and Machine Learning, Springer-Verlag, 2006</li>
<li>李航：统计学习方法，清华大学出版社，2012</li>
</ol>
<h1 id="related-Articles"><a href="#related-Articles" class="headerlink" title="related Articles"></a>related Articles</h1><ol start="0">
<li>Bayesian Theory</li>
<li>K-means</li>
</ol>
]]></content>
      <categories>
        <category>Machine_Learning</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>auto in CPP</title>
    <url>/auto_in_cpp/</url>
    <content><![CDATA[<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>auto&amp;, auto</p>
]]></content>
      <categories>
        <category>programming_languages</category>
      </categories>
      <tags>
        <tag>CPP</tag>
      </tags>
  </entry>
  <entry>
    <title>Publish Your WordPress Post in Automation with GitHub Actions</title>
    <url>/automated_wordpress_post/</url>
    <content><![CDATA[<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Python script:</p>
<ol>
<li>markdown with mathjax</li>
<li>render html with pandoc</li>
<li>Post to WordPress via restful API</li>
</ol>
<p>Trigger the above script through GitHub action.</p>
]]></content>
      <categories>
        <category>tools</category>
      </categories>
  </entry>
  <entry>
    <title>Write A Random Generator By Yourself</title>
    <url>/random_generator/</url>
    <content><![CDATA[<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1>]]></content>
      <categories>
        <category>Math</category>
      </categories>
      <tags>
        <tag>CPP</tag>
        <tag>Math</tag>
      </tags>
  </entry>
</search>

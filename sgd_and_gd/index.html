<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="en">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">






  
  
  <link rel="stylesheet" media="all" href="/lib/Han/dist/han.min.css?v=3.3">




<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />





  <link rel="alternate" href="/atom.xml" title="COMMENTARIUM" type="application/atom+xml" />






<meta name="description" content="IntroductionThis post mainly discusses the difference between gradient gescent (a.k.a. vanilla gradient descent or batch gradient descent) and stochastic gradient descent (a.k.a. online gradient desce">
<meta property="og:type" content="article">
<meta property="og:title" content="Difference Between Gradient Descent and Stochastic Gradient Descent">
<meta property="og:url" content="http://www.liuzhiwei.me/sgd_and_gd/index.html">
<meta property="og:site_name" content="COMMENTARIUM">
<meta property="og:description" content="IntroductionThis post mainly discusses the difference between gradient gescent (a.k.a. vanilla gradient descent or batch gradient descent) and stochastic gradient descent (a.k.a. online gradient desce">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2018-02-26T11:13:11.000Z">
<meta property="article:modified_time" content="2022-08-21T13:53:57.024Z">
<meta property="article:author" content="Zhiwei Liu">
<meta name="twitter:card" content="summary">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://www.liuzhiwei.me/sgd_and_gd/"/>





  <title>Difference Between Gradient Descent and Stochastic Gradient Descent | COMMENTARIUM</title>
  




  <script async src="https://www.googletagmanager.com/gtag/js?id=G-07WVZQ0WGT"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-07WVZQ0WGT');
</script>





<meta name="generator" content="Hexo 6.1.0"></head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">COMMENTARIUM</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/%20" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/%20" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.liuzhiwei.me/sgd_and_gd/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="COMMENTARIUM">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Difference Between Gradient Descent and Stochastic Gradient Descent</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-02-26T11:13:11+00:00">
                2018-02-26
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/machine-learning/" itemprop="url" rel="index">
                    <span itemprop="name">machine_learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body han-init-context" itemprop="articleBody">

      
      

      
        <h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>This post mainly discusses the difference between gradient gescent (a.k.a. vanilla gradient descent or batch gradient descent) and stochastic gradient descent (a.k.a. online gradient descent). We firstly show and compare the 2 alogorithms’ implementations, which is quite simple. After that, we will analyse these 2 algorithms from the convergence rate perspective. At the end, the summaries are given.</p>
<span id="more"></span>

<h1 id="Algorithm-Implementation"><a href="#Algorithm-Implementation" class="headerlink" title="Algorithm Implementation"></a>Algorithm Implementation</h1><p>In this part, we mainly concentrate on the implementation of both algorithms to get an intuive difference between them. Assumes we have loss function $L(\mathbf{w}; \mathbf{x})$, in which $\mathbf{w}$ is the parameters to train and $\mathbf{x}$ is the input data.<br>For gradient descent (GD),</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">gradient_descent</span>(<span class="params">loss, w, T, X, eta</span>):</span><br><span class="line">    :param loss: the loss function</span><br><span class="line">    :param w: trainable parameters</span><br><span class="line">    :param T: iterations to train</span><br><span class="line">    :param X: data points</span><br><span class="line">    :param eta: the learning rate</span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(T):</span><br><span class="line">        gradient_sum = []</span><br><span class="line">        <span class="keyword">for</span> x <span class="keyword">in</span> X:</span><br><span class="line">            cur_gradient = loss.subgradient(w, x)</span><br><span class="line">            gradient_sum = element_wise_add(gradient_sum, cur_gradient)</span><br><span class="line">        gradient = gradient_sum/X.size()</span><br><span class="line">        w = w - eta * gradient</span><br><span class="line">    <span class="keyword">return</span> w</span><br></pre></td></tr></table></figure>
<p>Note that in order to show the main difference with later SGD algorithm, above code snippet is not optimized (i.e. vectorized).</p>
<p>For stochastic gradient descent (SGD), </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">stochastic_gradient_descent</span>(<span class="params">loss, w, T, X, eta</span>):</span><br><span class="line">    :param loss: the loss function</span><br><span class="line">    :param w: trainable parameters</span><br><span class="line">    :param T: iterations to train</span><br><span class="line">    :param X: data points</span><br><span class="line">    :param eta: the learning rate</span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(T):</span><br><span class="line">        x = rand_choose(X)</span><br><span class="line">        cur_gradient = loss.subgradient(w, x)</span><br><span class="line">        w = w - eta * gradient</span><br><span class="line">    <span class="keyword">return</span> w</span><br></pre></td></tr></table></figure>
<p>As we could see from above 2 pseudo code snippets, the key difference is the SGD randomly chooses a data point to optimize, while the GD optimize on the whole dataset. As a result, the time complexity of GD and SGD are $\mathcal{O}(TN)$<br>(without considering vectorization) and $\mathcal{O}(T)$, respectively. Both implementations are simple and easily understood. </p>
<h1 id="Analysis"><a href="#Analysis" class="headerlink" title="Analysis"></a>Analysis</h1><p>This part analyses the convergence rate of the 2 algorithms mathematically. Since nonconvex optimization problems are extremely complex (NP-hard), we only discuss convex optimization here, which assumes the loss functions are convex.<br>The simplest case is that loss function $f(\mathbf{x})$ is $\gamma$-well-conditioned (which means the function is both $\alpha$-strongly convex and $\beta$-smooth).</p>
<h2 id="Gradient-Descent"><a href="#Gradient-Descent" class="headerlink" title="Gradient Descent"></a>Gradient Descent</h2><p>Set $h_t &#x3D; f(\mathbf{x}_t) - f(\mathbf{x}^{*})$, where $\mathbf{x}^*$ is the (unknown) optimal point. we will show that for unconstrained minimization: </p>
<p>$$h_{t+1} \leq h_{1}e^{-\gamma t}$$</p>
<p>in which $\eta_t &#x3D; \frac{1}{\beta}$.</p>
<p>According to strongly convexity, for any $\mathbf{x}, \mathbf{y} \in \mathcal{K}$:<br>$$<br>\begin{align}<br>f(\mathbf{y}) &amp; \geq f(\mathbf{x}) + \nabla f(\mathbf{x})^\top (\mathbf{y}-\mathbf{x}) + \frac{\alpha}{2} \Vert\mathbf{x} - \mathbf{y}\Vert^2 \<br>&amp; \geq \min_{\mathbf{z}}\Bigl\{f(\mathbf{x}) + \nabla f(\mathbf{x})^\top (\mathbf{z}-\mathbf{x}) + \frac{\alpha}{2} \Vert\mathbf{x} - \mathbf{z}\Vert^2\Bigr\} \<br>&amp; &#x3D; f(\mathbf{x}) - \frac{1}{2\alpha}\Vert\nabla f(\mathbf{x})\Vert^2<br>\end{align}<br>$$</p>
<p>In particular, take $\mathbf{x} &#x3D; \mathbf{x}_t$ and $\mathbf{y} &#x3D; \mathbf{x}^*$,</p>
<p>$$<br>\nabla f(\mathbf{x}_t) \geq 2\alpha(f(\mathbf{x}_t) - f(\mathbf{x}^*)) &#x3D; 2\alpha h_t<br>$$</p>
<p>Meanwhile, according to the $\beta$-smoothness,</p>
<p>$$<br>\begin{align}<br>h_{t+1} - h_t &amp; &#x3D; f(\mathbf{x}_{t+1}) - f(\mathbf{x}_t) \<br>&amp; \leq \nabla f(\mathbf{x}_t)^\top (\mathbf{x}_{t+1} - \mathbf{x}_{t}) + \frac{\beta}{2}\Vert \mathbf{x}_{t+1} - \mathbf{x}_t\Vert^2 \<br>&amp; &#x3D; -\eta_t \Vert \nabla f(\mathbf{x}_t) \Vert^2 + \frac{\beta}{2} \eta_t^2 \Vert \nabla f(\mathbf{x}_t) \Vert^2 \<br>&amp; &#x3D; - \frac{1}{2\beta}\Vert \nabla f(\mathbf{x}_t) \Vert^2 \<br>&amp; \leq -\frac{\alpha}{\beta}h_t<br>\end{align}<br>$$</p>
<p>Thus,</p>
<p>$$h_{t+1} \leq h_{t}(1-\frac{\alpha}{\beta} \leq \cdots \leq h_{1}(1-\gamma)^t \leq h_{1}e^{-\gamma t} \tag{2} \label{eq:2}$$</p>
<p>In order to make it comparable with later SGD analysis, we compute the <em>regret</em> $R_T &#x3D; \sum_{t&#x3D;1}^T f(\mathbf{x}_t) - \min_{x^* \in \mathcal{K}}\sum_{t&#x3D;1}^T f(\mathbf{x}^*) &#x3D; \sum_{t&#x3D;1}^T h_t$, according to $\ref{eq:2}$, </p>
<p>$$R_T \leq \frac{h_1}{e^\gamma - 1} (e^{-\gamma T} - 1) &#x3D; \mathcal{O}(e^{-\gamma T})$$</p>
<h2 id="Stochastic-Gradient-Descent"><a href="#Stochastic-Gradient-Descent" class="headerlink" title="Stochastic Gradient Descent"></a>Stochastic Gradient Descent</h2><p>SGD is an online optimizer, we use <em>regret</em> as measurement. Different with former mentioned formula, SGD’s regret is written as:</p>
<p>$$R_T &#x3D; \sum_{t&#x3D;1}^T f_t(\mathbf{x}_t) - \min_{\mathbf{x}^* \in \mathcal{K}} \sum_{t&#x3D;1}^T f_t(\mathbf{x}^*)$$</p>
<p>in which the fuction $f_t({\mathbf{x}})$ is different at each iteration since $\mathbf{x}$ are randomly chosed in each iteration in this algorithm. We can prove that </p>
<p>$$<br>R_T \leq \frac{3}{2}GD\sqrt{T}<br>$$<br>where $G$ is the lipschitz constant and $D$ is the diameter of decision set.</p>
<p>The 2 algorithms reprensents 2 different optimization schemes: offline optimization (GD) and online optimization (SGD).</p>
<h1 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h1><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p>[1] E. Hazan, “Introduction to Online Convex Optimization,” Found. Trends® Optim., vol. 2, no. 3–4, pp. 157–325, 2016.</p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/svm/" rel="next" title="Support Vector Machine">
                <i class="fa fa-chevron-left"></i> Support Vector Machine
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/automated_wordpress_post/" rel="prev" title="Publish Your WordPress Post in Automation with GitHub Actions">
                Publish Your WordPress Post in Automation with GitHub Actions <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name"></p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/%20%7C%7C%20archive">
              
                  <span class="site-state-item-count">14</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                
                  <span class="site-state-item-count">3</span>
                  <span class="site-state-item-name">categories</span>
                
              </div>
            

            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Introduction"><span class="nav-number">1.</span> <span class="nav-text">Introduction</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Algorithm-Implementation"><span class="nav-number">2.</span> <span class="nav-text">Algorithm Implementation</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Analysis"><span class="nav-number">3.</span> <span class="nav-text">Analysis</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Gradient-Descent"><span class="nav-number">3.1.</span> <span class="nav-text">Gradient Descent</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Stochastic-Gradient-Descent"><span class="nav-number">3.2.</span> <span class="nav-text">Stochastic Gradient Descent</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Summary"><span class="nav-number">4.</span> <span class="nav-text">Summary</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Reference"><span class="nav-number">5.</span> <span class="nav-text">Reference</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Zhiwei Liu</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
